from os import environ
from openai import OpenAI
from typing import Any

import instructor
from tenacity import Retrying, stop_after_attempt, wait_fixed


def get_instructor_client(model: str) -> instructor.client.Instructor:
    """
    Returns an instructor client based on the given model.

    Args:
        model (str): The name of the model to use. If the model starts with "gpt-", an OpenAI client is returned.
        Otherwise, a local Ollama client is returned.

    Returns:
        instructor.client.Instructor: The instructor client based on the model.
    """
    # Get either an openai or an ollama client, based on model
    if model.startswith("gpt-"):
        # Assume we are using OpenAI
        # Get openai key from environment if available, otherwise use creds.py

        OPENAI_KEY = environ.get("OPENAI_KEY", environ.get("OPENAI_API_KEY"))
        if OPENAI_KEY is None:
            from creds import OPENAI_KEY

        client = instructor.from_openai(
            OpenAI(
                api_key=OPENAI_KEY,
            )
        )
    else:
        # We are using local Ollama
        client = instructor.from_openai(
            OpenAI(
                base_url="http://localhost:11434/v1",
                api_key="ollama",  # required, but unused
            ),
            mode=instructor.Mode.JSON,
        )

    return client


def simple_completion(
    prompt: str | list[str] | list[dict[str, str]] | dict[str, str],
    *,
    model: str,
    client: instructor.client.Instructor | None = None,
    responseclass: type | None = None,
    stop: int = 2,
    wait: int = 1,
) -> str | Any:
    """
    Generate a simple completion using the given prompt and model.

    Args:
        prompt (str | list[dict[str, str]] | dict[str, str]): The prompt to generate the completion from. It can be a string, a list of strings, or a dictionary.
        model (str): The name of the model to use for the completion.
        client (instructor.client.Instructor | None, optional): The client to use for the completion. If not provided, a client will be created based on the model. Defaults to None.
        responseclass (type | None, optional): The class to use for the completion response. If not provided, the response will be a string. Defaults to None.
        stop (int, optional): The maximum number of times to retry the completion. Defaults to 2.
        wait (int, optional): The number of seconds to wait between retries. Defaults to 1.

    Returns:
        str | Any: The completion generated by the model. If `responseclass` is provided, the completion will be an instance of that class. Otherwise, it will be a string.

    Raises:
        TypeError: If the `prompt` argument is not of type str, list, or dict.
    """
    # If no client given, get either an openai or an ollama client, based on model
    if client is None:
        client = get_instructor_client(model=model)

    # Create full prompt based on type of what given
    if isinstance(prompt, str):
        # If prompt is a string, create a list with a single user message
        messages = [{"role": "user", "content": prompt}]
    elif isinstance(prompt, list):
        # If prompt is a list, check if it contains strings or dictionaries
        if isinstance(prompt[0], str):
            # If it contains strings, create a list of user messages
            messages = [{"role": "user", "content": p} for p in prompt]
        else:
            # If it contains dictionaries, use the prompt as is
            messages = prompt
    elif isinstance(prompt, dict):
        # If prompt is a dictionary, create a list with a single user message
        messages = [prompt]
    else:
        raise TypeError("Prompt must be of type str, list, or dict")

    # Create completion
    response = client.chat.completions.create(
        model=model,
        response_model=responseclass,
        messages=messages,
        max_retries=Retrying(
            stop=stop_after_attempt(stop),  # Maximum number of retries
            wait=wait_fixed(wait),  # Number of seconds to wait between retries
        ),
    )

    # If no responseclass given we will have a ChatCompletion object, so extract the string message from that:
    if responseclass is None:
        response = response.choices[0].message.content

    return response


def chunk_list(
    string_list: list[str],
    max_len: int,
    joiner: str = "\n",
) -> list[str]:
    """
    Concatenates a list of strings with a specified joiner and chunks them into smaller chunks of length less than `max_len`,
    while keeping the original strings intact.

    Args:
        string_list (List[str]): The list of strings to concatenate and chunk.
        max_len (int): The maximum length of each chunk.
        joiner (str, optional): The string used to join the original strings. Defaults to '\n'.

    Returns:
        List[str]: The list of chunks.
    """
    # Initialize the list of chunks to return
    chunks: list[str] = []

    # Initialize the current chunk
    current_chunk: str = ""

    # Iterate over the list of strings
    for string in string_list:
        # If the current chunk is close to max_len, create a new chunk
        if len(current_chunk) + len(joiner) + len(string) > max_len:
            # If the current chunk is not empty, add it to the list of chunks
            if current_chunk:
                chunks.append(current_chunk)
            # Add the current string as a new chunk
            chunks.append(string)
            # Reset the current chunk
            current_chunk = ""
        else:
            # If the current chunk is not empty, add the joiner
            if current_chunk:
                current_chunk += joiner
            # Add the current string to the current chunk
            current_chunk += string

    # If the current chunk is not empty, add it to the list of chunks
    if current_chunk:
        chunks.append(current_chunk)

    # Return the list of chunks
    return chunks
